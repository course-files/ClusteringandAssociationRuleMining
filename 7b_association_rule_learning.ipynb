{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Association Rule Learning using the Apriori Algorithm\n",
    "\n",
    "| Key              | Value                                                                                                                                                                                                                                                                                                        |\n",
    "|:-----------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Course Codes** | BBT 4206 and BFS 4102                                                                                                                                                                                                                                                                                        |\n",
    "| **Course Names** | BBT 4206: Business Intelligence II (Week 1-3 of 13) and<br/>BFS 4102: Advanced Business Data Analytics (Week 4-6 of 13)                                                                                                                                                                                      |\n",
    "| **Semester**     | August to November 2025                                                                                                                                                                                                                                                                                      |\n",
    "| **Lecturer**     | Allan Omondi                                                                                                                                                                                                                                                                                                 |\n",
    "| **Contact**      | aomondi@strathmore.edu                                                                                                                                                                                                                                                                                       |\n",
    "| **Note**         | The lecture contains both theory and practice.<br/>This notebook forms part of the practice.<br/>It is intended for educational purposes only.<br/>Recommended citation: [BibTex](https://raw.githubusercontent.com/course-files/ClusteringandAssociationRuleMining/refs/heads/main/RecommendedCitation.bib) |\n",
    "\n",
    "**Business context:** A supermarket chain seeks to uncover frequent item combinations from historical transactions to improve product placement, plan promotions, and increase cross-selling.\n",
    "\n",
    "**Dataset:** The **\"groceries\"** dataset by **Hahsler et al. (2011)** contains 9,835 market basket transactions. Each row in the dataset represents items in a customer's shopping cart."
   ],
   "id": "741572eed43762dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 1: Import the necessary libraries",
   "id": "7581ebe5c7fc59cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Purpose**: This chunk imports all the necessary libraries for data analysis, machine learning, and visualization.\n",
    "\n",
    "1. **For File and system operations [urllib3](https://urllib3.readthedocs.io/en/stable/)**\n",
    "    - `urllib.request` is used for opening and downloading data from URLs.\n",
    "    - `from pathlib import Path` is used to handle file paths in a platform-independent way.\n",
    "    - `os` provides functions for interacting with the operating system, such as file and directory management.\n",
    "\n",
    "2. **For data manipulation - [pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/overview.html):**\n",
    "    - `pandas as pd`: For loading the dataset, creating and managing DataFrames, data manipulation and analysis using DataFrames\n",
    "\n",
    "3. **For data preprocessing and transformation - [mlxtend](https://rasbt.github.io/mlxtend/)**\n",
    "    - `TransactionEncoder`: Converts transaction data into a binary matrix format\n",
    "\n",
    "4. **For association rule learning - [mlxtend](https://rasbt.github.io/mlxtend/)**\n",
    "    - `apriori`: Implements the Apriori algorithm for finding frequent itemsets\n",
    "    - `association_rules`: Generates rules from frequent itemsets\n",
    "\n",
    "5. **For data visualization - [matplotlib](https://matplotlib.org/stable/gallery/index.html) and [seaborn](https://seaborn.pydata.org/examples/index.html)**\n",
    "    - `matplotlib.pyplot as plt`: For basic plotting functionality\n",
    "    - `seaborn as sns`: For enhanced statistical visualizations\n",
    "\n",
    "6. **For suppressing warnings - [warnings](https://docs.python.org/3/library/warnings.html)**\n",
    "    - `warnings`: Controls warning messages\n",
    "    - `warnings.filterwarnings('ignore')`: Suppresses warning messages for cleaner output\n",
    "    - Used to suppress warnings that may arise during the execution of the code. Even though it is not necessary for the code to run, it helps in keeping the output clean and focused on the results."
   ],
   "id": "462d352864ff9375"
  },
  {
   "cell_type": "code",
   "id": "7f7e4ec8",
   "metadata": {},
   "source": [
    "# For file and system operations\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# For data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# For data preprocessing and transformation\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# For Association Rule Learning\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# For data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For suppressing warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6a5d5cc0",
   "metadata": {},
   "source": "## Step 2: Load the data"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. **Data Loading Process**\n",
    "    - Uses a context manager (`with` statement) for proper resource handling\n",
    "    - Steps:\n",
    "        1. Opens URL connection\n",
    "        2. Reads binary content\n",
    "        3. Decodes from bytes to UTF-8 string\n",
    "        4. Splits into lines and processes each line\n",
    "\n",
    "2. **Data Processing**\n",
    "    - List comprehension transforms raw data into a structured format\n",
    "    - Each transaction is processed by:\n",
    "        - `strip()`: Removes whitespace\n",
    "        - `split(',')`: Creates a list of items from comma-separated values\n",
    "\n",
    "3. **Output**\n",
    "    - Prints the total number of transactions\n",
    "    - Shows the first three transactions as a sample\n"
   ],
   "id": "e7796a6675a67683"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset_path = './data/groceries.csv'\n",
    "url = 'https://github.com/course-files/ClusteringandAssociationRuleMining/raw/refs/heads/main/data/groceries.csv'\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(\"Downloading dataset...\")\n",
    "    if not os.path.exists('./data'):\n",
    "        os.makedirs('./data')\n",
    "    urllib.request.urlretrieve(url, dataset_path)\n",
    "    print(\"✅ Dataset downloaded\")\n",
    "else:\n",
    "    print(\"✅ Dataset already exists locally\")\n",
    "\n",
    "path = Path(dataset_path)\n",
    "content = path.read_text(encoding='utf-8')\n",
    "\n",
    "transactions = []   # Start with an empty list\n",
    "\n",
    "# Go through each line in the text\n",
    "for line in content.splitlines():\n",
    "    # Remove spaces and check if the line is not empty\n",
    "    if line.strip():\n",
    "        items = []  # Start a new shopping cart (list of items)\n",
    "\n",
    "        # Go through each item in this line\n",
    "        for item in line.split(\",\"):\n",
    "            # Remove spaces and ignore empty items\n",
    "            if item.strip():\n",
    "                items.append(item.strip())\n",
    "\n",
    "        # Add this cleaned shopping cart to the transactions list\n",
    "        transactions.append(items)\n",
    "\n",
    "print(f\"Total number of transactions: {len(transactions)}\")\n",
    "print(\"\\nFirst three transactions:\")\n",
    "transactions[:3]"
   ],
   "id": "7ac2253053ab1281",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b6f5d2b1",
   "metadata": {},
   "source": "## Step 3: Convert the transaction list into a one-hot encoded DataFrame"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Purpose:**\n",
    "\n",
    "The purpose of this chunk is to format the transaction data correctly. This is essential for:\n",
    "- Running the Apriori algorithm\n",
    "- Computing association rules\n",
    "- Analyzing item frequencies and relationships\n",
    "\n",
    "1. **Transaction Encoder Initialization**\n",
    "    - Creates a new `TransactionEncoder` object called `encoder`\n",
    "    - Purpose: To convert transaction lists into a binary matrix format\n",
    "\n",
    "2. **Fit and Transform Process**\n",
    "    - `encoder.fit(transactions)`:\n",
    "        - Learns all unique items across all transactions\n",
    "        - Creates a mapping of items to columns\n",
    "\n",
    "    - `transform(transactions)`:\n",
    "        - Converts transactions into a binary matrix\n",
    "        - Each row represents one transaction\n",
    "        - Each column represents one item\n",
    "        - Values: True/False indicating item presence\n",
    "\n",
    "3. **DataFrame Creation**\n",
    "    - Converts the binary matrix into a `pandas` DataFrame\n",
    "    - Uses `encoder.columns_` to retrieve the column names\n",
    "    - Each column name is a unique item\n",
    "    - Each row shows items present (True) or absent (False)\n",
    "\n",
    "4. **Data Preview**\n",
    "    - `transaction_data.head()`: Shows first five rows of transformed data\n",
    "\n",
    "**Example:**\n",
    "If the original transactions were:\n",
    "```\n",
    "Transaction 1: [\"milk\", \"bread\"]\n",
    "Transaction 2: [\"bread\", \"butter\"]\n",
    "```\n",
    "The transformed data would look like this:\n",
    "```\n",
    "   milk  bread  butter\n",
    "0  True   True  False\n",
    "1  False  True   True\n",
    "```"
   ],
   "id": "eebaf02bf6039263"
  },
  {
   "cell_type": "code",
   "id": "dc60c753",
   "metadata": {},
   "source": [
    "encoder = TransactionEncoder()\n",
    "onehot = encoder.fit(transactions).transform(transactions)\n",
    "transaction_data = pd.DataFrame(onehot, columns=encoder.columns_)\n",
    "transaction_data.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6650a767",
   "metadata": {},
   "source": "## Step 4: Generate frequent `itemsets` using the Apriori algorithm"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. **Apriori Algorithm Application**\n",
    "    - Function: `apriori()`\n",
    "    - Parameters:\n",
    "        - `transaction_data`: Binary encoded transaction matrix\n",
    "        - `min_support=0.02`: Minimum support threshold (2%)\n",
    "        - `use_colnames=True`: Use item names instead of indices\n",
    "\n",
    "2. **Support Threshold**\n",
    "    - Support(itemset X) = (number of transactions containing itemset X) / (total transactions)\n",
    "    - 0.02 means itemset X must appear in at least 2% of transactions\n",
    "    - Helps filter out rare combinations\n",
    "\n",
    "3. **Result Generation**\n",
    "    - Creates a DataFrame containing:\n",
    "        - `itemsets`: Combinations of items\n",
    "        - `support`: Frequency of occurrence\n",
    "\n",
    "4. **Result Processing**\n",
    "    - `sort_values(by='support', ascending=False)`: Orders by support value (highest first)\n",
    "    - `head(10)`: Shows top 10 most frequent itemsets"
   ],
   "id": "200d6dd2edc69577"
  },
  {
   "cell_type": "code",
   "id": "d50fea9b",
   "metadata": {},
   "source": [
    "frequent_itemsets = apriori(transaction_data, min_support=0.02, use_colnames=True)\n",
    "frequent_itemsets.sort_values(by='support', ascending=False).head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "de5afbbb",
   "metadata": {},
   "source": "## Step 5: Generate and display the association rules"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. **Rules Generation**\n",
    "    - Function: `association_rules()`\n",
    "    - Input: `frequent_itemsets` from Apriori algorithm\n",
    "    - Parameters:\n",
    "        - `metric='lift'`: A measure of how much more often items in the antecedent and the items in the consequent appear together in transactions compared to what would be expected if they were statistically independent.\n",
    "        - `min_threshold=1.0`: Minimum lift value to include\n",
    "\n",
    "2. **Rules and Key Metrics**\n",
    "    - `antecedents`: \"If\" part of the rule (items in a basket)\n",
    "    - `consequents`: \"Then\" part of the rule (likely additional items)\n",
    "    - `support`: Frequency of items appearing together\n",
    "    - `confidence`: Probability of consequent given antecedent\n",
    "    - `lift`: Ratio of observed support to expected support\n",
    "\n",
    "3. **Sorting and Display**\n",
    "    - Sorts rules by confidence (highest first)\n",
    "    - Shows the top 10 strongest associations\n",
    "    - Displays most relevant columns for analysis"
   ],
   "id": "f24277983ab5546c"
  },
  {
   "cell_type": "code",
   "id": "d735dc2f",
   "metadata": {},
   "source": "rules = association_rules(frequent_itemsets, metric='lift', min_threshold=1.0)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Display the top 10 rules sorted by confidence",
   "id": "1c61bdf5fbc1ef5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rules = rules.sort_values(by='confidence', ascending=False)\n",
    "rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10)"
   ],
   "id": "fdf94effdae3dbaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Display the top 10 rules sorted by lift and then confidence",
   "id": "43f10e589b3b2b3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rules = rules.sort_values(by=['lift', 'confidence'], ascending=[False, False])\n",
    "rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10)"
   ],
   "id": "de29263a81c18154",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filter the rules to show only those with high confidence and high lift",
   "id": "19174d6802073dec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "strong_rules = rules[\n",
    "    (rules['confidence'] >= 0.2) &\n",
    "    (rules['lift'] >= 2.0)\n",
    "].sort_values(by='confidence', ascending=False)\n",
    "strong_rules.head(15)[['antecedents', 'consequents', 'support', 'confidence', 'lift']]"
   ],
   "id": "e7f4045568588e16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 6: Remove duplicate and redundant rules",
   "id": "a216039302b85e33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_duplicates(rules_df):\n",
    "    rules_df = rules_df.sort_values(by=['lift', 'confidence'], ascending=[False, False]).reset_index(drop=True)\n",
    "    unique_rules = []\n",
    "\n",
    "    for i, row_i in rules_df.iterrows():\n",
    "        is_redundant = False\n",
    "        for j, row_j in enumerate(unique_rules):\n",
    "            if row_i['consequents'] == row_j['consequents'] and row_i['antecedents'].issubset(row_j['antecedents']):\n",
    "                is_redundant = True\n",
    "                break\n",
    "\n",
    "        if not is_redundant:\n",
    "            unique_rules.append(row_i)\n",
    "\n",
    "    return pd.DataFrame(unique_rules)\n",
    "\n",
    "# Apply the function to the strong rules\n",
    "nonredundant_rules = remove_duplicates(strong_rules)\n",
    "nonredundant_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]"
   ],
   "id": "fe248b54d5343a62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rules = rules.sort_values(by=['lift', 'confidence'], ascending=[False, False])\n",
    "rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10)"
   ],
   "id": "5e96de623677e864",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Eliminate bidirectional redundancy in rules\n",
    "def remove_bidirectional_redundancy(rules_df):\n",
    "    # Create a set to track unique antecedent-consequent pairs\n",
    "    seen_rules = set()\n",
    "    filtered_rules = []  # Store the final filtered rules\n",
    "\n",
    "    for _, row in rules_df.iterrows():\n",
    "        # Combine antecedents and consequents into a frozenset so the order doesn't matter\n",
    "        rule_pair = frozenset([frozenset(row['antecedents']), frozenset(row['consequents'])])\n",
    "\n",
    "        # Only keep the rule if it hasn't already been seen\n",
    "        if rule_pair not in seen_rules:\n",
    "            seen_rules.add(rule_pair)\n",
    "            filtered_rules.append(row)\n",
    "\n",
    "    return pd.DataFrame(filtered_rules)\n",
    "\n",
    "# Apply the function to the strong rules\n",
    "cleaned_rules = remove_bidirectional_redundancy(nonredundant_rules)\n",
    "cleaned_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]"
   ],
   "id": "9cde956e08cf5d8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**List of Rules**",
   "id": "2ca2e161b91fd7f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cleaned_rules = cleaned_rules.sort_values(by=['lift', 'confidence'], ascending=[False, False])\n",
    "cleaned_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10)"
   ],
   "id": "8b33ffd869fd36ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 7: Save the rules as a CSV file",
   "id": "496ccfbf416bafca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the output path\n",
    "output_path = './rule/top_rules_7b.csv'\n",
    "\n",
    "# Ensure the data directory exists\n",
    "if not os.path.exists('./rule'):\n",
    "    os.makedirs('./rule')\n",
    "\n",
    "# Save the top rules as a CSV file\n",
    "cleaned_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10).to_csv(output_path, index=False)\n",
    "print(f\"\\n✅ Top rules saved to {output_path}\")\n",
    "\n",
    "# Provide a download link if running in Google Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(output_path)\n",
    "except ImportError:\n",
    "    print(\"❌ Not running in Google Colab, skipped rule download link.\")"
   ],
   "id": "f10b55780c2bb996",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 8: Visualize rules",
   "id": "58c13c64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.scatterplot(data=rules, x='support', y='confidence', size='lift', hue='lift', palette='viridis', sizes=(30, 300))\n",
    "plt.title('Support vs Confidence of Association Rules')\n",
    "plt.xlabel('Support')\n",
    "plt.ylabel('Confidence')\n",
    "plt.grid(True)\n",
    "plt.legend(title='Lift', bbox_to_anchor=(1.15, 1), loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "6952eb68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.scatterplot(data=rules, x='lift', y='confidence', size='lift', hue='lift', palette='viridis', sizes=(30, 300))\n",
    "plt.title('Lift vs Confidence of Association Rules')\n",
    "plt.xlabel('Lift')\n",
    "plt.ylabel('Confidence')\n",
    "plt.grid(True)\n",
    "plt.legend(title='Lift', bbox_to_anchor=(1.15, 1), loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "88c9739afbf62601",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# References\n",
    "Hahsler, M., Chelluboina, S., Hornik, K., & Buchta, C. (2011). The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Datasets. Journal of Machine Learning Research, 12, 1977–1981.\n"
   ],
   "id": "2c0a87e3056f186b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lab Submission\n",
    "\n",
    "## Student Details\n",
    "\n",
    "**Name of the team on GitHub Classroom:**\n",
    "\n",
    "**Team Member Contributions**\n",
    "\n",
    "**Member 1**\n",
    "\n",
    "| **Details**                                                                                        | **Comment** |\n",
    "|:---------------------------------------------------------------------------------------------------|:------------|\n",
    "| **Student ID**                                                                                     |             |\n",
    "| **Name**                                                                                           |             |\n",
    "| **What part of the lab did you personally contribute to,** <br>**and what did you learn from it?** |             |\n",
    "\n",
    "**Member 2**\n",
    "\n",
    "| **Details**                                                                                        | **Comment** |\n",
    "|:---------------------------------------------------------------------------------------------------|:------------|\n",
    "| **Student ID**                                                                                     |             |\n",
    "| **Name**                                                                                           |             |\n",
    "| **What part of the lab did you personally contribute to,** <br>**and what did you learn from it?** |             |\n",
    "\n",
    "**Member 3**\n",
    "\n",
    "| **Details**                                                                                        | **Comment** |\n",
    "|:---------------------------------------------------------------------------------------------------|:------------|\n",
    "| **Student ID**                                                                                     |             |\n",
    "| **Name**                                                                                           |             |\n",
    "| **What part of the lab did you personally contribute to,** <br>**and what did you learn from it?** |             |\n",
    "\n",
    "**Member 4**\n",
    "\n",
    "| **Details**                                                                                        | **Comment** |\n",
    "|:---------------------------------------------------------------------------------------------------|:------------|\n",
    "| **Student ID**                                                                                     |             |\n",
    "| **Name**                                                                                           |             |\n",
    "| **What part of the lab did you personally contribute to,** <br>**and what did you learn from it?** |             |\n",
    "\n",
    "**Member 5**\n",
    "\n",
    "| **Details**                                                                                        | **Comment** |\n",
    "|:---------------------------------------------------------------------------------------------------|:------------|\n",
    "| **Student ID**                                                                                     |             |\n",
    "| **Name**                                                                                           |             |\n",
    "| **What part of the lab did you personally contribute to,** <br>**and what did you learn from it?** |             |\n",
    "\n",
    "## Instructions\n",
    "\n",
    "You are working as a data analyst for a retail store. Using the association rules derived from the dataset (`cleaned_rules`), you are required to provide business recommendations and design a simple recommender function.\n",
    "\n",
    "### Part 1: Business Analysis (Critical Thinking)\n",
    "\n",
    "- Examine the association rules in `cleaned_rules`.\n",
    "- Present a business analysis report that addresses:\n",
    "  - Which rules are useful and actionable for the business? (e.g., cross-promotions, store layout, bundling strategies).\n",
    "  - Which rules are misleading or not useful despite appearing strong? (Consider support, confidence, and lift).\n",
    "  - Recommend two specific business actions the company should implement based on your findings.\n",
    "\n",
    "### Part 2: Recommender System (Programming)\n",
    "\n",
    "**Baseline (Required)**\n",
    "- Create a function called `dynamic_recommender_baseline(cart, rules_df)` that:\n",
    "  - Accepts a list of products in the client’s cart as the antecedent.\n",
    "  - Searches `cleaned_rules` for matching antecedents.\n",
    "  - Returns the consequent(s) as the recommendations.\n",
    "\n",
    "**Intermediate (Recommended)**\n",
    "- Extend your function by creating another one named `dynamic_recommender_intermediate(cart, rules_df)` that is extended to:\n",
    "  - Handle multiple items in the shopping cart.\n",
    "  - If no rule matches, return \"No recommendation available.\"\n",
    "  - Rank recommendations using confidence (highest first).\n",
    "\n",
    "**Advanced (Optional Challenge)**\n",
    "- Enhance your recommender by creating another function named `dynamic_recommender_advanced(cart, rules_df)` that is extended to:\n",
    "  - Allowing the user to request top-N recommendations.\n",
    "  - Incorporating lift to break ties between recommendations.\n",
    "  - Comparing your recommender’s output to a naïve baseline (e.g., always recommend top-selling products).\n",
    "  - Briefly discuss: When would association rule recommenders fail in real businesses?\n",
    "\n",
    "### Deliverables\n",
    "\n",
    "1. Business Analysis Report.\n",
    "2. Python implementation of the `dynamic_recommender` function. State whether you completed the Baseline, Intermediate, or Advanced version.\n",
    "3. Sample runs of your function showing different shopping carts and outputs.\n",
    "\n",
    "### Grading Approach\n",
    "\n",
    "- Baseline = Pass (implementation works, basic analysis) >= 60%\n",
    "- Intermediate = Merit (handles edge cases, thoughtful analysis) 75–85%\n",
    "- Advanced = Distinction (robust function, strategic business insights) >= 86%\n",
    "\n",
    "**Note 1:** The real challenge is not building the function—it is deciding which rules actually matter for the business. A good analyst filters the noise, questions misleading correlations, and translates data into profitable actions.\n",
    "\n",
    "**Note 2:** The sophistication of the recommender is not measured by code complexity, but by how well it handles real-world edge cases—like sparse data, missing rules, and ambiguous ties."
   ],
   "id": "39e79f118d2db095"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
